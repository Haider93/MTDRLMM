--- 
data: 
  md_dir: 
    - C:/Users/computer_name/Desktop/data_mm_nn/dia/train/md_dir
  symbols: 
    - HSBA.L
  tas_dir: 
    - C:/Users/computer_name/Desktop/data_mm_nn/dia/train/tas_dir
debug: 
  inspect_books: false
  random_seed: 100
evaluation: 
  md_dir: 
    - C:/Users/computer_name/Desktop/data_mm_nn/amzn/test/md_dir
  symbols: 
    - HSBA.L
  n_samples: 20
  tas_dir: 
    - C:/Users/computer_name/Desktop/data_mm_nn/amzn/test/tas_dir
  random_agent: false
  use_train_sample: false
learning: 
  algorithm: sarsa
  alpha_floor: 0.001
  alpha_start: 0.001
  beta: 0.005
  gamma: 0.975
  group_weights: 
    - 0.65
    - 0.25
    - 0.1
  lambda: 0.85
  memory_size: 20000000
  n_actions: 9
  n_tilings: 8
  omega: 1.0
  weight_incr: 0.0001
  python_module: dqn_test_MTL
logging: 
  log_backtest: true
  log_learning: true
  max_size: 500000
market: 
  latency: 
    floor: 0.0
    mu: 0.0
    sigma: 0.0
    type: fixed
  order_size: 10
  pos_lb: -100
  pos_ub: 100
  target_price: 
    lookback: 1
    type: target_price
  transaction_fee: 0.0
output_dir: "C://Users//computer_name//Desktop//"
policy: 
  eps_T: 800
  eps_floor: 0.0001
  eps_init: 0.5
  spread_lookback: 45
  type: epsilon_greedy
reward: 
  damping_factor: 0.7
  measure: pnl_damped
  pnl_lookback: 0
  pnl_weight: 1.0
  pos_weight: 0.0
state: 
  lookback: 
    mpm: 15
    rsi: 0
    svl: 60
    vlt: 60
    vwap: 0
  variables: 
    - pos
    - a_dist
    - b_dist
    - mpm
    - spd
    - vol
    - imb
    - svl
training: 
  n_episodes: 1000
  n_samples: 1
  n_threads: 1
